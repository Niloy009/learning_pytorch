import matplotlib.pyplot as plt
import torch
import torchvision
from torch import nn
from torchvision import transforms
from torchmetrics import Accuracy

try:
    from torchinfo import summary
except:
    print(f"[INFO] Couldn't find the torchinfo...\n Installing it.....")
    !pip install -qq torchinfo
    from torchinfo import summary

# Setup Summary writer
try:
    from torch.utils.tensorboard import SummaryWriter
except:
    print(f'[INFO]: Could not find tensorboard..... installing it!! ')
    !pip install -qq tensorboard
    from torch.utils.tensorboard import SummaryWriter

# Try to import going_moduler directory. download it from GitHub, if it doesn't work
try:
    from going_modular import data_setup, engine, utils
except:
    print(f"[INFO] Couldn't find the directory...\n Downloading it from github.....")
    !git clone https://github.com/Niloy009/learning_pytorch.git
    !mv leaning_pytorch/going_modular .
    !rm -rf learning_pytorch
    from going_modular import data_setup, engine, utils


# Setup device agnostic code
device = 'cuda' if torch.cuda.is_available() else "cpu"
device


# Set the seed
def set_seeds(seed: int=42):
    """Sets random sets for torch operations.

    Args:
        seed (int, optional): Random seed to set. Defaults to 42.
    """

    # set the seed for the general torch operation
    torch.manual_seed(seed)

    # Set the seed for CUDA torch operations (ones that happen on the GPU)
    torch.cuda.manual_seed(seed)





import os
import zipfile

from pathlib import Path

import requests



def download_data(source: str, 
                  destination: str, 
                  remove_source: bool = True) -> Path:
    """Download a ziped dataset from source and unzip to destination

    Args:
        source: The source path where the data will download from.
        destination: The destination path where the data will download and unzip to.
        remove_source: Whether the source remove or not after download.
        
    Returns:
        pathlib.Path to downloaded data.
    """


    # Setup data path
    data_path = Path("data/")
    image_path = data_path / destination # images from a subset of classes from the Food101 dataset

    # If the image folder doesn't exist, download it and prepare it...
    if image_path.is_dir():
      print(f"[INFO] {image_path} directory exists, skipping re-download.")
    else:
      print(f"[INFO] Did not find {image_path}, downloading it...")
      image_path.mkdir(parents=True, exist_ok=True)

      # Download pizza, steak, sushi data
      target_file = Path(source).name
      with open(data_path / target_file, "wb") as f:
        request = requests.get(source)
        print(f"[INFO] Downloading {target_file} from {source}...")
        f.write(request.content)
  
      # unzip pizza, steak, sushi data
      with zipfile.ZipFile(data_path / target_file, "r") as zip_ref:
        print(f"[INFO] Unzipping {target_file}...")
        zip_ref.extractall(image_path)
  
      # Remove .zip file
      if remove_source:
          os.remove(data_path / target_file)

    return image_path


image_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip", 
              destination="pizza_steak_sushi")





# Setup directories
train_dir = image_path / "train"
test_dir = image_path / "test"

train_dir, test_dir


# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])

# Create transform pipeline manually
manual_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    normalize
])           
print(f"[INFO] Manually created transforms: {manual_transforms}")

train_dataloader_manual, test_dataloaler_manual, class_names = data_setup.create_dataloaders(train_dir=train_dir, 
                                                                                test_dir=test_dir, 
                                                                                transform=manual_transforms, 
                                                                                batch_size=32)
train_dataloader_manual, test_dataloaler_manual, class_names





# Setup pretrained weights (plenty of these available in torchvision.models)
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT

# Get transforms from weights (these are the transforms that were used to obtain the weights)
automatic_transforms = weights.transforms() 
print(f"[INFO] Automatically created transforms: {automatic_transforms}")


train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir, 
                                                                                test_dir=test_dir, 
                                                                                transform=automatic_transforms, 
                                                                                batch_size=32)
train_dataloader, test_dataloader, class_names





# Download the weights of pretrained model Efficientnet_B0
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT

# Set up the model with weights and send it to the device
model = torchvision.models.efficientnet_b0(weights=weights).to(device)

# View the model
# model


# Freeze all base layers by setting attribute required_grad to False
for param in model.features.parameters():
    param.requires_grad = False

# Since we're creating a new layer with random weights (torch.nn.Linear), 
# let's set the seeds
set_seeds()

model.classifier = nn.Sequential(
    nn.Dropout(p=0.2, inplace=True),
    nn.Linear(in_features=1280, out_features=len(class_names), bias=True)
)



# Get a summary of the model
summary(model=model, 
        input_size=(32,3,224,224), 
        verbose=0, 
        col_names=['input_size', 'output_size', 'num_params', 'trainable'], 
        col_width=18, row_settings=['var_names'])






# Define loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)
accuracy = Accuracy(task='multiclass', num_classes=len(class_names)).to(device)



writer = SummaryWriter()
writer



import torchmetrics
from tqdm.auto import tqdm
from typing import Dict, List, Tuple

from going_modular.engine import train_step, test_step


def train(model: torch.nn.Module, 
          train_dataloader: torch.utils.data.DataLoader, 
          test_dataloader: torch.utils.data.DataLoader, 
          loss_fn: torch.nn.Module, 
          optimizer: torch.optim.Optimizer,
          accuracy: torchmetrics.classification.accuracy.Accuracy,
          epochs: int,
          device: torch.device) -> Dict[str, List]:
    """Trains and test a PyTorch model

    Passes a target PyTorch model through train_step() and test_step()
    functions for a number of epochs. training and testing the model in the same epoch loop.

    Calculates, prints and stores evaluation metrics throughout.

    Args:
        model: A PyTorch model to be tested.
        train_dataloader: A DataLoader instance for the model to be trained on.
        test_dataloader: A DataLoader instance for the model to be tested on.
        loss_fn: A PyTorch loss function to calculate loss on the test data.
        optimizer: A PyTorch optimizer to help minimize the loss function.
        accuracy: A torchmetric module to calculate accuracy.
        epochs: An integar indicating how many epochs to train for.
        device: A target device to compute on (i.e. "cuda" or "cpu")

    Returns:
        A dictionary of training and testing loss as well as training and
        testing accuracy metrics. Each metric has a value in a list for 
        each epoch.
        In the form: {train_loss: [...],
                      train_acc: [...],
                      test_loss: [...],
                      test_acc: [...]} 
        For example if training for epochs=2: 
                     {train_loss: [2.0616, 1.0537],
                      train_acc: [0.3945, 0.3945],
                      test_loss: [1.2641, 1.5706],
                      test_acc: [0.3400, 0.2973]} 
  """
    # Create empty results dictionary
    results = { "train_loss": [], 
                "train_accuracy": [], 
                "test_loss": [], 
                "test_accuracy": []
              }

    # Loop through training and testing steps for a number of epochs
    for epoch in tqdm(range(epochs)):
        train_loss, train_accuracy = train_step(model=model, 
                                                dataloader=train_dataloader, 
                                                loss_fn=loss_fn, 
                                                optimizer=optimizer, 
                                                accuracy=accuracy, 
                                                device=device)
        test_loss, test_accuracy = test_step(model=model, 
                                             dataloader=test_dataloader, 
                                             loss_fn=loss_fn,
                                             accuracy=accuracy, 
                                             device=device)
        
        print(
            f"Epoch: {epoch+1} | "
            f"train_loss: {train_loss: .4f} | "
            f"train_accuracy: {train_accuracy: .4f} | "
            f"test_loss: {test_loss: .4f} | "
            f"test_accuracy: {test_accuracy: .4f}"
        )
        # 5. update the results
        results["train_loss"].append(train_loss)
        results["train_accuracy"].append(train_accuracy)
        results["test_loss"].append(test_loss)
        results["test_accuracy"].append(test_accuracy)

        #### New: Experiment tracking with tensorboard ####
        writer.add_scalars(main_tag="Loss", 
                           tag_scalar_dict={"train_loss": train_loss, 
                                            "test_loss": test_loss}, 
                           global_step=epoch)
        
        writer.add_scalars(main_tag="Accuracy", 
                           tag_scalar_dict={"train_accuracy": train_accuracy, 
                                            "test_accuracy": test_accuracy}, 
                           global_step=epoch)

        writer.add_graph(model=model, input_to_model=torch.randn(32,3,224,224).to(device))

        # Close the writer
        writer.close()

        #### End: Experiment tracking with tensorboard ####
        

    return results




# Train Model
# Note: Not using engine.train() as we modified the function above

set_seeds()
results = train(model=model, 
                train_dataloader=train_dataloader, 
                test_dataloader=test_dataloader, 
                loss_fn=loss_fn, 
                optimizer=optimizer,
                accuracy=accuracy, 
                epochs=5, 
                device=device)



# Lets view our experiment
# %load_ext tensorboard
# %tensorboard --logdir runs





from datetime import datetime
import os

from torch.utils.tensorboard import SummaryWriter

def create_writer(experiment_name: str, model_name: str, extra: str = None) -> torch.utils.tensorboard.writer.SummaryWriter():
    """Create a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir
    
    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra

    Where timestamp is current date in YYYY-MM-DD format

    Args:
        experiment_name (str): Name of the experiment.
        model_name (str): Name of the model
        extra (str, optional): Anything extra to add to the directory.

    Returns:
        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to the specific log_dir.

    Example usage:
        # Create a writer saving to "runs/2025-Apr-05/data_10_percent/effnetb2/5_epochs/"
        writer = create_writer(experiment_name="data_10_percent",
                               model_name="effnetb2",
                               extra="5_epochs")
        # The above is the same as:
        writer = SummaryWriter(log_dir="runs/2022-06-04/data_10_percent/effnetb2/5_epochs/")
    """

    # Get timestamp of current date in reverse order (YYYY-MM-DD)
    timestamp = datetime.now().strftime("%Y-%b-%d")

    if extra:
        # create log directory path
        log_dir = os.path.join('runs', timestamp, experiment_name, model_name, extra)
    else:
        log_dir = os.path.join('runs', timestamp, experiment_name, model_name)

    print(f"[INFO] Created SummaryWriter saving to {log_dir}")
    return SummaryWriter(log_dir=log_dir)
    


example_writer = create_writer(experiment_name='data_10_percent', model_name="efficientnetb0", extra='5_epochs')
example_writer


import torchmetrics
from tqdm.auto import tqdm
from typing import Dict, List, Tuple

from going_modular.engine import train_step, test_step


def train(model: torch.nn.Module, 
          train_dataloader: torch.utils.data.DataLoader, 
          test_dataloader: torch.utils.data.DataLoader, 
          loss_fn: torch.nn.Module, 
          optimizer: torch.optim.Optimizer,
          accuracy: torchmetrics.classification.accuracy.Accuracy,
          epochs: int,
          device: torch.device, 
          writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List]:
    """Trains and test a PyTorch model

    Passes a target PyTorch model through train_step() and test_step()
    functions for a number of epochs. training and testing the model in the same epoch loop.

    Calculates, prints and stores evaluation metrics throughout.
    
    Stores metrics to specified writer log_dir if present.
    
    Args:
        model: A PyTorch model to be tested.
        train_dataloader: A DataLoader instance for the model to be trained on.
        test_dataloader: A DataLoader instance for the model to be tested on.
        loss_fn: A PyTorch loss function to calculate loss on the test data.
        optimizer: A PyTorch optimizer to help minimize the loss function.
        accuracy: A torchmetric module to calculate accuracy.
        epochs: An integar indicating how many epochs to train for.
        device: A target device to compute on (i.e. "cuda" or "cpu").
        writer: A SummaryWriter() instance to log model results to.


    Returns:
        A dictionary of training and testing loss as well as training and
        testing accuracy metrics. Each metric has a value in a list for 
        each epoch.
        In the form: {train_loss: [...],
                      train_acc: [...],
                      test_loss: [...],
                      test_acc: [...]} 
        For example if training for epochs=2: 
                     {train_loss: [2.0616, 1.0537],
                      train_acc: [0.3945, 0.3945],
                      test_loss: [1.2641, 1.5706],
                      test_acc: [0.3400, 0.2973]} 
  """
    # Create empty results dictionary
    results = { "train_loss": [], 
                "train_accuracy": [], 
                "test_loss": [], 
                "test_accuracy": []
              }

    # Loop through training and testing steps for a number of epochs
    for epoch in tqdm(range(epochs)):
        train_loss, train_accuracy = train_step(model=model, 
                                                dataloader=train_dataloader, 
                                                loss_fn=loss_fn, 
                                                optimizer=optimizer, 
                                                accuracy=accuracy, 
                                                device=device)
        test_loss, test_accuracy = test_step(model=model, 
                                             dataloader=test_dataloader, 
                                             loss_fn=loss_fn,
                                             accuracy=accuracy, 
                                             device=device)
        
        print(
            f"Epoch: {epoch+1} | "
            f"train_loss: {train_loss: .4f} | "
            f"train_accuracy: {train_accuracy: .4f} | "
            f"test_loss: {test_loss: .4f} | "
            f"test_accuracy: {test_accuracy: .4f}"
        )
        # 5. update the results
        results["train_loss"].append(train_loss)
        results["train_accuracy"].append(train_accuracy)
        results["test_loss"].append(test_loss)
        results["test_accuracy"].append(test_accuracy)

        #### New: Experiment tracking with tensorboard ####
        if writer:
            writer.add_scalars(main_tag="Loss", 
                               tag_scalar_dict={"train_loss": train_loss, 
                                                "test_loss": test_loss}, 
                               global_step=epoch)
            
            writer.add_scalars(main_tag="Accuracy", 
                               tag_scalar_dict={"train_accuracy": train_accuracy, 
                                                "test_accuracy": test_accuracy}, 
                               global_step=epoch)
    
            writer.add_graph(model=model, input_to_model=torch.randn(32,3,224,224).to(device))
    
            # Close the writer
            writer.close()
        else:
            pass

        #### End: Experiment tracking with tensorboard ####
        

    return results
















# Download 10 percent and 20 percent datasets
data_10_percent_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi.zip", 
                                destination="pizza_steak_sushi_10")

data_20_percent_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi_20_percent.zip", 
                                    destination='pizza_steak_sushi_20')







# Get the train and test path
train_10_dir = data_10_percent_path / 'train'
train_20_dir = data_20_percent_path / 'train'

test_10_dir = data_10_percent_path / 'test'

train_10_dir, train_20_dir, test_10_dir


def create_transform_weights(model_weight):
    """ Create transforms by using pretrained model best weights
    Args:
        model_weight: A pretrained model's weight from torchvision
    Return:
        A tuple of weights and transforms
    """
    # Setup pretrained weights
    weights = model_weight.DEFAULT

    transforms = weights.transforms()

    print(f'[INFO] Creating Transform from pretrained model best weights: {transforms}')

    return weights, transforms



# Setup the Batch Size
BATCH_SIZE = 32

# Setup best weights and transforms based on best weights
effnet_b0_weights, effnet_b0_transform = create_transform_weights(model_weight=torchvision.models.EfficientNet_B0_Weights)

# # Setup pretrained weights
# effnet_b0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT

# # Get the transform of the pretrained model best weights
# effnet_b0_transform = effnet_b0_weights.transforms()
# print(f'[INFO] Creating Transform from pretrained model best weights: {effnet_b0_transform}')

# Create the dataloaders for 10%
train_dataloader_10, test_datalaoder, class_names = data_setup.create_dataloaders(train_dir=train_10_dir, 
                                                                                     test_dir=test_10_dir, 
                                                                                     transform=effnet_b0_transform, 
                                                                                     batch_size=BATCH_SIZE)
# Create the dataloaders for 20%
train_dataloader_20, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_20_dir, 
                                                                                 test_dir=test_10_dir, 
                                                                                 transform=effnet_b0_transform, 
                                                                                 batch_size=BATCH_SIZE)


print(f'[INFO] Number of batches of size {BATCH_SIZE} in 10% train data: {len(train_dataloader_10)}')
print(f'[INFO] Number of batches of size {BATCH_SIZE} in 20% train data: {len(train_dataloader_20)}')
print(f'[INFO] Number of batches of size {BATCH_SIZE} in 10% test data: {len(test_dataloader)}')
print(f'[INFO] Class names: {class_names} & length of the class: {len(class_names)}')






import torchvision
from torch import nn

# Get num of the output features
OUT_FEATURE = len(class_names)


# Create an EffnetB0 feature extractor
def create_effnetb0():
    # get the pretrained weights nad base model
    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
    model = torchvision.models.efficientnet_b0(weights=weights).to(device)

    # Freeze the base model layers
    for param in model.features.parameters():
        param.requires_grad = False

    # set the seeds
    set_seeds()

    # Change the classifier head
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.2, inplace=True),
        nn.Linear(in_features=1280, out_features=OUT_FEATURE)
    ).to(device)
    
    # give the model name
    model.name = 'effnetb0'
    print(f"[INFO] Created new {model.name} model")
    return model

# Create an EffnetB2 feature extractor
def create_effnetb2():
    # get the pretrained weights nad base model
    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT
    model = torchvision.models.efficientnet_b2(weights=weights).to(device)

    # Freeze the base model layers
    for param in model.features.parameters():
        param.requires_grad = False

    # set the seeds
    set_seeds()

    # Change the classifier head
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.3, inplace=True),
        nn.Linear(in_features=1408, out_features=OUT_FEATURE)
    ).to(device)
    
    # give the model name
    model.name = 'effnetb2'
    print(f"[INFO] Created new {model.name} model")
    return model


effnetb0_model = create_effnetb0()
effnetb2_model = create_effnetb2()

summary(model=effnetb0_model, 
        input_size=(32, 3, 224, 224), 
        verbose=0, 
        col_names=['input_size', 'output_size', 'num_params', 'trainable'], 
        col_width=18, 
        row_settings=['var_names'])


summary(model=effnetb2_model, 
        input_size=(32, 3, 224, 224), 
        verbose=0, 
        col_names=['input_size', 'output_size', 'num_params', 'trainable'], 
        col_width=18, 
        row_settings=['var_names'])





# Create epoch list
num_epochs = [5,10]

# Create model list (need to create a new mdoel for each experiment)
models = ['effnetb0', 'effnetb2']

# Create a DataLoaders dictionary
train_dataloaders = {'data_10_percent': train_dataloader_10, 
                     'data_20_percent':  train_dataloader_20}


%%time
from going_modular.utils import save_model
from going_modular import engine

# Set Seeds
set_seeds()

# Keep track of experiment numbers
experiment_number = 0

# Loop through each DataLoaders
for dataloader_name, train_dataloader in train_dataloaders.items():
    # Loop through the epochs
    for epochs in num_epochs:
        # Loop through the model name and Create a new model instance
        for model_name in models:
            
            experiment_number += 1
            
            # Print out the info
            print(f"[INFO] Experminet Number: {experiment_number}")
            print(f"[INFO] Model: {model_name}")
            print(f"[INFO] DataLoader: {dataloader_name}")
            print(f"[INFO] Number of Epochs: {epochs}")

            # Select and Create the model
            if model_name == 'effnetb0':
                model = create_effnetb0()
            else:
                model = create_effnetb2()
            
            # Create a new loss, optimizer and accuracy for every model
            loss_fn = nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)
            accuracy = Accuracy(task='multiclass', num_classes=len(class_names)).to(device)

            # Train target model with target dataloader and track experiment
            engine.train(model=model, 
                         train_dataloader=train_dataloader, 
                         test_dataloader=test_dataloader, 
                         loss_fn=loss_fn, 
                         optimizer=optimizer, 
                         accuracy=accuracy, 
                         epochs=epochs, 
                         device=device, 
                         writer=create_writer(experiment_name=dataloader_name, 
                                              model_name=model_name, extra=f"{epochs}_epochs"))
            
            # Save the model to file so we can import it later if needed
            save_file_path = f"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth"
            save_model(model=model, target_dir='models', model_name=save_file_path)
            print("-"*50 + "\n")  


!nvidia-smi
!kill 45432


!kill 45432


# Lets view our experiment
# %reload_ext tensorboard
# %tensorboard --logdir runs --port=6007








# Setup best model path
best_model_path = 'models/07_effnetb2_data_20_percent_10_epochs.pth'

# Instantiate a new instance of effnetb2 (to load in the saved state_dict())
best_model = create_effnetb2()

# Load the saved best model state_dict()
best_model.load_state_dict(torch.load(best_model_path))


# Check the model file size

# Get the model size in bytes then convert it to MB
effnetb2_model_size = Path(best_model_path).stat().st_size // (1024 *1024)
print(f'[INFO] EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB')





from going_modular import predictions
import random

# Get a random list of 3 image path names from the test dataset
num_images_to_plot = 4
test_image_path_list = list(Path(data_20_percent_path / 'test').glob("*/*.jpg"))
test_image_path_sample = random.sample(test_image_path_list, k=num_images_to_plot)

for image_path in test_image_path_sample:
    predictions.pred_and_plot_image(model=best_model, 
                                    image_path=image_path, 
                                    class_names=class_names, 
                                    image_size=(224,224))






data_path = Path('data/')
for x in list(Path(data_path).glob('*.jpg')):
    predictions.pred_and_plot_image(model=best_model, image_path=x, class_names=class_names, image_size=(224,224))
